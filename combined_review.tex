\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comprehensive Review of Deep Reinforcement Learning Strategies for UAV Navigation and Conflict Resolution: A 16-Study Analysis}

\author{\IEEEauthorblockN{Harshvardhan Pandey}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
harshpandey145@gmail.com}
\and
\IEEEauthorblockN{Adya Chauhan}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
adyachauhan.04@gmail.com}
\and
\IEEEauthorblockN{Yash Prasad}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
eyash.prasad24@gmail.com}
}

\maketitle

\begin{abstract}
The autonomous navigation of Unmanned Aerial Vehicles (UAVs) in complex, dynamic, and unknown environments remains a critical challenge, particularly in the absence of reliable communication or global positioning. Traditional path planning algorithms (e.g., A*, RRT) often struggle with real-time adaptability and computational efficiency in high-dimensional state spaces. Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm to address these limitations by enabling end-to-end learning of control policies directly from sensor inputs. This paper presents an extensive, in-depth review of sixteen cutting-edge studies on DRL-based and hybrid UAV navigation systems. We dissect each approach based on network architecture, reward function design, training methodologies, and quantitative performance metrics. Furthermore, we provide a comprehensive comparative analysis using standardized metrics such as algorithm class, state-space dimensionality, and simulation environments. Our synthesis reveals a growing trend towards hybrid architectures combining DRL with classical planners, the use of attention mechanisms for better feature extraction, and the critical role of reward shaping in convergence. Finally, we identify key gaps in current research, particularly the scarcity of real-world validation and the need for more robust sim-to-real transfer protocols.
\end{abstract}

\begin{IEEEkeywords}
Unmanned Aerial Vehicles (UAV), Deep Reinforcement Learning (DRL), Path Planning, Obstacle Avoidance, Neural Networks, Autonomous Navigation
\end{IEEEkeywords}

\section{Introduction}
\subsection{Background and Motivation}
The proliferation of Unmanned Aerial Vehicles (UAVs) across civil, commercial, and military domains has necessitated robust autonomous navigation systems capable of operating without human intervention. From package delivery in dense urban canyons to search-and-rescue missions in disaster zones, UAVs must perceive their environment, plan trajectories, and execute control commands in real-time. While classical methods like Artificial Potential Fields (APF) and Rapidly-exploring Random Trees (RRT) offer theoretical guarantees in static maps, they often fail in dynamic, partially observable environments due to their inability to generalize to unforeseen scenarios.

Deep Reinforcement Learning (DRL) integrates the function approximation power of Deep Neural Networks (DNNs) with Reinforcement Learning (RL) principles, allowing agents to learn complex policies from high-dimensional sensory data (e.g., RGB images, LiDAR point clouds). Algorithms such as Deep Q-Networks (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) have shown remarkable success. However, applying DRL to UAVs introduces unique challenges: continuous action spaces, strict safety constraints, limited on-board computational resources, and the "sim-to-real" gap.

\subsection{Objectives and Scope}
This review significantly expands upon previous surveys by providing a granular analysis of sixteen distinct approaches. Unlike high-level overviews, we delve into the specific implementation details---neural network layers, hyperparameter choices, and exact reward formulations---of each study. Our objective is to provide researchers with a technical blueprint of what works, what fails, and why.

The reviewed works are categorized into:
\begin{itemize}
    \item \textbf{Value-Based Methods:} Focus on learning the Q-value function (e.g., DQN variants).
    \item \textbf{Policy-Based/Actor-Critic Methods:} Learn the policy directly, often suitable for continuous control (e.g., DDPG, TD3, PPO).
    \item \textbf{Hybrid Approaches:} Combine DRL with classical motion planners or supervised learning.
    \item \textbf{Non-DRL Benchmarks:} Heuristic and information-theoretic approaches for comparison.
\end{itemize}

\section{Detailed Analysis of Methodologies}

\subsection{Value-Based Approaches (DQN Family)}

\subsubsection{FRDDM-DQN [1]}
Wang et al. propose a framework integrating Faster R-CNN for object detection with a DQN for decision-making. The network comprises three fully connected layers (512, 256, 128 units) with ReLU activation. The reward function $R = R_{goal} + R_{col} + R_{step}$ is standard, but the innovation lies in the Data Deposit Mechanism (DDM), which prioritizes "important" experiences (e.g., near-collisions) in the replay buffer. This addressed the issue of sparse rewards in large environments, achieving a 94\% success rate.

\subsubsection{Curriculum-Based Dueling DQN [3]}
Carvalho et al. tackle the complexity of 3D urban environments using Curriculum Learning. Instead of a single hard task, the agent is trained on a sequence of increasingly difficult environments (varying obstacle density). The architecture is a Dueling DQN, which separates the estimation of State Value $V(s)$ and Advantage $A(s,a)$, improving stability. The input is a depth map processed by 3 CNN layers. Results show a reduction in convergence time by 30\%.

\subsubsection{Agile DQN with Attention [8]}
AlMahamid et al. introduce "Agile DQN," which incorporates a Visual Attention mechanism (Glimpse Network) and an LSTM layer. The attention module allows the UAV to focus processing resources on relevant parts of the image (obstacles), rather than the entire frame. This reduced inference time by 20\% and improved success rates in dynamic environments to 92\%.

\subsubsection{Improved DQN with Self-Supervised Learning [9]}
Samma et al. address the sample inefficiency of RL. They use a two-stage training process: first, a ResNet-18 backbone is pre-trained using a self-supervised task (predicting the next frame) to learn rich feature representations without reward signals. Then, the network is fine-tuned with Dueling DQN. This approach reduced the training steps required to reach 90\% success by 40\%.

\subsection{Policy-Based \& Actor-Critic Approaches}

\subsubsection{Adaptive Multi-Agent PPO [2]}
Fremond et al. apply Multi-Agent PPO (MAPPO) to the problem of tactical conflict resolution in shared airspace. By using a centralized critic during training and decentralized actors during execution (CTDE), agents learned cooperative behaviors. A GRU (Gated Recurrent Unit) layer was essential for handling the partial observability of other agents' intents. The system resolved 98.5\% of conflicts.

\subsubsection{Guide Attention TD3 (GARTD3) [5]}
Yin et al. enhance the Twin Delayed DDPG (TD3) algorithm with an attention mechanism for processing LiDAR data. The reward function includes specific terms for angular velocity $r_{ang}$ and linear velocity $r_{vel}$ to ensure smooth flight. GARTD3 reduced the collision rate by 40\% compared to vanilla TD3 in narrow corridors.

\subsubsection{TD3 for Local Path Planning [11]}
Zhao et al. utilize TD3 for local obstacle avoidance. The actor network (400-300 units) outputs continuous velocity commands. The study highlights the effectiveness of TD3's "clipped double Q-learning" in reducing the overestimation bias common in DDPG, achieving a 93\% success rate in Gazebo simulations.

\subsubsection{Safety-Aware PPO (SIGN) [10]}
Yan et al. propose SIGN, which uses a Lagrangian relaxation method to embed safety constraints directly into the optimization objective. Unlike standard rewards where safety is just a penalty, SIGN treats it as a hard constraint. This resulted in a collision rate of $<2\%$, significantly lower than unconstrained PPO.

\subsubsection{Real-World DDPG for USV [12]}
Woo et al. present one of the few studies with real-world validation, albeit for a Surface Vehicle (USV). The DDPG agent was trained in simulation and transferred to a physical boat. The reward function penalized rapid action changes to prevent mechanical wear. Real-world tests confirmed sub-meter path following accuracy.

\subsection{Hybrid and Non-DRL Approaches}

\subsubsection{RLPlanNav (Hierarchical) [4]}
Xue and Chen combine a high-level DQN planner (which selects sub-goals) with a low-level gradient-based trajectory optimizer (EGO-Planner). This hybrid approach leverages the global reasoning of RL and the precise local control of classical planners, achieving 100\% success in static environments where pure RL might get stuck.

\subsubsection{V-Diagram \& Crowding (Heuristic) [15]}
Chen et al. use a geometric V-Diagram for initial planning and a "crowding mechanism" for local avoidance. While lacking the learning capability of DRL, this heuristic method demonstrated extremely fast reaction times (<50ms) to sudden threats, highlighting the computational efficiency of non-learning methods.

\section{Comparative Analysis}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{comparison_assets/radar_chart.png}
\caption{Radar Chart comparing the performance profiles of different algorithm classes. Hybrid methods offer a balance of safety and efficiency, while heuristic methods excel in compute efficiency.}
\label{fig:radar}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{comparison_assets/reward_components.png}
\caption{Frequency of Reward Function Components. Goal and Collision rewards are universal, but only a subset of papers incorporate smoothness or energy constraints.}
\label{fig:rewards}
\end{figure}

Table I and Table II provide a detailed side-by-side comparison of all 16 studies.

\begin{table*}[htbp]
\caption{Comprehensive Comparison of UAV Navigation Algorithms (Part 1: Methodology \& Environment)}
\label{tab:comp_part1}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{Study} & \textbf{Algorithm Class} & \textbf{Core Algorithm} & \textbf{State Space} & \textbf{Action Space} & \textbf{Simulator} & \textbf{Real-World?} \\
\hline
[1] Wang et al. & Value-Based & FRDDM-DQN & RGB + Pos & Discrete & Custom (OpenGL) & No \\
\hline
[2] Fremond et al. & Policy-Based & MAPPO (RNN) & Relative Pos & Continuous & Custom (Urban) & No \\
\hline
[3] Carvalho et al. & Value-Based & Dueling DQN & Depth Map & Discrete & AirSim & No \\
\hline
[4] Xue \& Chen & Hybrid (Hierarchical) & DQN + Planner & Occupancy Grid & Discrete (High-level) & Gazebo & No \\
\hline
[5] Yin et al. & Policy-Based & GARTD3 (Attention) & Laser Range & Continuous & Gazebo & No \\
\hline
[6] Duan et al. & Non-DRL & Frontier Planner & Voxel Map & Discrete (Grid) & Custom & No \\
\hline
[7] Wang et al. & Value-Based & DQN (Dynamic Reward) & Sensor Data & Discrete & Custom & No \\
\hline
[8] AlMahamid et al. & Value-Based & Agile DQN (RNN) & RGB (Glimpses) & Discrete & Unity ML-Agents & No \\
\hline
[9] Samma et al. & Hybrid (SSL+RL) & Improved DQN & RGB & Discrete & AirSim & No \\
\hline
[10] Yan et al. & Policy-Based (Safe) & PPO-Lagrangian & RGB-D & Continuous & Habitat & No \\
\hline
[11] Zhao et al. & Policy-Based & TD3 & Laser Range & Continuous & Gazebo & No \\
\hline
[12] Woo et al. & Policy-Based & DDPG & Error States & Continuous & Custom + Real & \textbf{Yes} \\
\hline
[13] Wu et al. & Policy-Based & Multi-Critic DDPG & State Vector & Continuous & MATLAB & No \\
\hline
[14] Xie et al. & Value-Based & DRQN & POMDP State & Discrete & Unity & No \\
\hline
[15] Chen et al. & Non-DRL & V-Diagram & Threat Info & Continuous & Custom & No \\
\hline
[16] Cheng et al. & Non-DRL & Info Theoretic & Local Info & Discrete & Custom & No \\
\hline
\end{tabular}
}
\end{center}
\end{table*}

\begin{table*}[htbp]
\caption{Comprehensive Comparison of UAV Navigation Algorithms (Part 2: Performance \& Details)}
\label{tab:comp_part2}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Study} & \textbf{Reward Function Components} & \textbf{Network / Method Details} & \textbf{Learning Rate} & \textbf{Key Metric} & \textbf{Result} \\
\hline
[1] Wang et al. & Goal(+), Collision(-), Step(-) & Faster R-CNN + 3-layer DQN & 0.00025 & Success Rate & 94\% \\
\hline
[2] Fremond et al. & Conflict(-), Goal(+), Smooth(-) & GRU-based Policy (64 units) & 0.0005 (Est) & Conflict Resolution & 98.5\% \\
\hline
[3] Carvalho et al. & Goal(+), Potential(+), Energy(-) & Dueling DQN (Conv Layers) & Dynamic & Success Rate & 91\% (Dense) \\
\hline
[4] Xue \& Chen & Progress(+), Safety(-), Valid(+) & Hierarchical (DQN + Gradient) & 0.0001 & Success Rate & 100\% (Static) \\
\hline
[5] Yin et al. & Goal, Col, Angular, Velocity & LSTM Actor + Attention & 0.001 & Collision Rate & -40\% vs TD3 \\
\hline
[6] Duan et al. & N/A (Cost Function) & Multi-Res Octree & N/A & Compute Time & -60\% \\
\hline
[7] Wang et al. & Safety Envelope, Dist Change & DQN + Action Prob & Unknown & Success Rate & 95\% \\
\hline
[8] AlMahamid et al. & Goal, Col, Time, Glimpse(-) & Glimpse Net + LSTM (256) & 0.0001 & Success Rate & 92\% \\
\hline
[9] Samma et al. & Goal, Col, Shaping & ResNet-18 + Dueling DQN & 0.0001 & Training Steps & -40\% steps \\
\hline
[10] Yan et al. & Task(+), Safety Cost & PPO-Lagrangian & 0.0003 & Collision Rate & <2\% \\
\hline
[11] Zhao et al. & Dist, Heading, Obstacle(-) & TD3 (400, 300 units) & 0.001 & Success Rate & 93\% \\
\hline
[12] Woo et al. & Cross-track, Heading, Action & DDPG (400, 300 units) & 0.0001 & Tracking Error & <0.5m \\
\hline
[13] Wu et al. & Distance Reward & Multi-Critic (3 Critics) & Unknown & Convergence & +20\% Speed \\
\hline
[14] Xie et al. & FOV Reward, Nav Reward & DRQN (LSTM) & 0.00025 & Success Rate & 88\% \\
\hline
[15] Chen et al. & N/A (Heuristic) & Cubic Spline + Crowding & N/A & Reaction Time & <50ms \\
\hline
[16] Cheng et al. & Information Gain & Info Utility Maximization & N/A & Coverage & 95\% \\
\hline
\end{tabular}
}
\end{center}
\end{table*}

\section{Discussion and Future Directions}
\subsection{The Dominance of Hybrid Architectures}
Our analysis indicates a clear trend: pure end-to-end DRL often struggles with long-horizon planning and precise control. Hybrid architectures (e.g., [4]) that offload the "strategic" decisions to RL and the "tactical" control to classical planners appear to offer the highest reliability.

\subsection{Importance of Attention and Memory}
Given the partial observability of onboard sensors, memory mechanisms like LSTM/GRU ([2], [5], [8], [14]) are becoming standard. Furthermore, attention mechanisms are proving crucial for processing high-dimensional visual data efficiently on resource-constrained UAV hardware.

\subsection{The Reality Gap}
Only one of the sixteen reviewed studies [12] included real-world validation. This "sim-to-real" gap remains the most significant hurdle. Future research must prioritize domain randomization and real-world testing to validate the robustness of DRL policies trained in simulation.

\section{Conclusion}
This comprehensive review analyzed sixteen state-of-the-art studies in UAV navigation. By dissecting the architectural choices, reward functions, and performance metrics, we provided a detailed landscape of the current field. While DRL offers unparalleled adaptability, the path forward likely lies in hybrid systems that combine the learning capability of neural networks with the safety guarantees of control theory.

\begin{thebibliography}{00}
\bibitem{b1} Wang et al., "Deep-reinforcement-learning-based UAV autonomous navigation..."
\bibitem{b2} Fremond et al., "Adaptive Multi-Agent Reinforcement Learning..."
\bibitem{b3} Carvalho et al., "UAV Navigation in 3D Urban Environments..."
\bibitem{b4} Xue and Chen, "Combining Motion Planner and Deep Reinforcement Learning..."
\bibitem{b5} Yin et al., "Autonomous UAV Navigation with Adaptive Control..."
\bibitem{b6} Duan et al., "Multi-Level-Frontier Empowered Adaptive Path Planning..."
\bibitem{b7} J. Wang et al., "APPA-3D: an autonomous 3D path planning algorithm...", Nature Sci Rep, 2024.
\bibitem{b8} F. AlMahamid et al., "Agile DQN: adaptive deep recurrent attention...", Nature Sci Rep, 2024.
\bibitem{b9} H. Samma et al., "Autonomous UAV Visual Navigation Using an Improved DRL", IEEE Access, 2024.
\bibitem{b10} Y. Yan et al., "SIGN: Safety-Aware Image-Goal Navigation...", Arxiv, 2025.
\bibitem{b11} Z. Feiyu et al., "Autonomous localized path planning... based on TD3", Nature Sci Rep, 2024.
\bibitem{b12} J. Woo et al., "Deep reinforcement learning-based controller for path following of a USV", Ocean Eng, 2019.
\bibitem{b13} R. Wu et al., "A multi-critic deep deterministic policy gradient UAV path planning", IEEE CIS, 2020.
\bibitem{b14} R. Xie et al., "UAV Path Planning... in Large-Scale and Dynamic Environments", IEEE Access, 2021.
\bibitem{b15} X. Chen et al., "Collaborative Path Planning... to Avoid Sudden Threats", Shenyang Aerospace.
\bibitem{b16} H. Cheng et al., "Cooperative control of UAV swarm via information measures", IJIUS.
\end{thebibliography}

\end{document}
