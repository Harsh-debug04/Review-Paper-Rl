\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{A Review on Deep Reinforcement Learning-Based UAV Navigation and Conflict Resolution in Unknown Environments (Expanded)}

\author{\IEEEauthorblockN{Harshvardhan Pandey}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
harshpandey145@gmail.com}
\and
\IEEEauthorblockN{Adya Chauhan}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
adyachauhan.04@gmail.com}
\and
\IEEEauthorblockN{Yash Prasad}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence and Machine Learning} \\
\textit{Manipal University Jaipur}\\
Jaipur, India \\
eyash.prasad24@gmail.com}
}

\maketitle

\begin{abstract}
Unmanned Aerial Vehicles or UAVs have become very popular. They have found applications in diverse fields including surveillance, disaster response, logistics, and inspections of unsafe situations and defense missions. However, their ability to navigate through complex, dynamic environments in case of communication loss is important. Traditional approaches lack adaptability and are computationally expensive. In response to this, researchers have integrated Deep Reinforcement Learning (DRL) techniques. This paper reviews sixteen studies on DRL-based and hybrid approaches for UAV navigation and collision avoidance. They are analyzed based on problem formulation, approaches, architectural innovations, performance, and real-world applications. A comparative study is provided evaluating their strengths, limitations, and potential for future research.
\end{abstract}

\begin{IEEEkeywords}
unmanned aerial vehicles, deep reinforcement learning, navigation, obstacle avoidance, collision resolution, autonomous systems
\end{IEEEkeywords}

\section{Introduction}
\subsection{Background and Motivation}
Unmanned aerial vehicles (UAVs), widely known as drones, have come to occupy a vital place in today's automation environment. Their compactness and easy maneuverability at an ever-decreasing cost have opened their applications into the domains of military, commercial, and industrial. One such huge challenge is the autonomous navigation of these vehicles in entirely unknown or partly known environments. Classical path generation techniques like A*, Rapidly exploring random trees (RRT), and artificial potential fields have the limitation of being dependent on pre-existing maps and are computationally inefficient in dynamic environments. They are designed and trained for static ideal-world scenarios, making them unreliable in unknown environments.

Deep reinforcement learning (DRL) is a promising paradigm that has transformed the approach to solving navigation problems. This approach allows agents to learn optimal policies through experiences accumulated in simulated environments, notwithstanding the criteria of partial observability and dynamic constraints. Thus, DRL has allowed navigation policy learning that signifies selection of optimal policy based on prior training. Integrating high-dimensional inputs such as images and LIDAR data while accommodating real-time decision-making without exhaustive path computation makes these models ideal for dynamic environments.

\subsection{Objectives and Scope}
This review critically analyzes sixteen key papers, including the original six studies and ten additional works:

1) Deep-reinforcement-learning-based UAV autonomous navigation and collision avoidance in unknown environments (FRDDM-DQN) [1]
2) Adaptive Multi-Agent Reinforcement Learning Solver for Tactical Conflict Resolution [2]
3) UAV Navigation in 3D Urban Environments with Curriculum-based Deep Reinforcement Learning [3]
4) Combining Motion Planner and Deep Reinforcement Learning for UAV Navigation in Unknown Environment [4]
5) Autonomous UAV Navigation with Adaptive Control Based on Deep Reinforcement Learning [5]
6) Multi-Level-Frontier Empowered Adaptive Path Planning for UAVs in Unknown Environments [6]
7) APPA-3D: an autonomous 3D path planning algorithm for UAVs in unknown complex environments [7]
8) Agile DQN: adaptive deep recurrent attention reinforcement learning for autonomous UAV obstacle avoidance [8]
9) Autonomous UAV Visual Navigation Using an Improved Deep Reinforcement Learning [9]
10) SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning [10]
11) Autonomous localized path planning algorithm for UAVs based on TD3 strategy [11]
12) Deep reinforcement learning-based controller for path following of an unmanned surface vehicle [12]
13) A multi-critic deep deterministic policy gradient UAV path planning [13]
14) Unmanned Aerial Vehicle Path Planning Algorithm Based on Deep Reinforcement Learning in Large-Scale and Dynamic Environments [14]
15) Collaborative Path Planning for Multiple Unmanned Aerial Vehicles to Avoid Sudden Threats [15]
16) Cooperative control of UAV swarm via information measures [16]

Each paper is analyzed with respect to problem formulation, model architecture, training methodology, evaluation metrics, and experimental outcomes.

\section{Literature Review}
The integration of Deep Reinforcement Learning (DRL) in the field of Unmanned Aerial Vehicles has attracted significant attention particularly based on partially observable and dynamic environments. Wang et al. [1] proposed the FRDDM-DQN framework that integrated Faster R-CNN for visual obstacle detection and DQN. Fremond et al. [2] utilized MARL with PPO for tactical conflict resolution. Carvalho et al. [3] implemented curriculum learning in DQN. Xue and Chen [4] combined DRL with a motion planner. Yin et al. [5] introduced GARTD3 with attention mechanisms. Duan et al. [6] described a non-learning-based method using multi-level octree voxelization.

Beyond these, recent advancements include Wang et al. [7] with APPA-3D using dynamic rewards, AlMahamid et al. [8] proposing Agile DQN with attention mechanisms, and Samma et al. [9] introducing a two-stage learning process with self-supervised learning. Other notable works explore safety-aware navigation [10], TD3 strategies for local planning [11], and multi-critic architectures [13]. While most focus on DRL, some studies explore heuristic collaborative planning [15] and information-theoretic decentralized control [16], providing a broader perspective on the field.

\section{In-Depth Analysis of Selected Works}

\subsection{FRDDM-DQN for UAV Navigation and Obstacle Avoidance [1]}
\subsubsection{Problem Statement}
This work is concerned with autonomous UAV navigation in GPS-denied environments in which traditional sensors, like radar, are compromised. The challenge is obstacle avoidance using visual information.
\subsubsection{Methodology}
The authors introduce a hybrid architecture including Faster R-CNN for visual detection, DQNs for learning policies, and a Data Deposit Mechanism (DDM) for prioritizing experience replay. Training occurs in a 3D simulation.
\subsubsection{Innovations}
Use of DDM to retain safety-critical experiences and a two-stage training pipeline.
\subsubsection{Results}
Outperformed YOLO-based methods and DQN variants, achieving improved navigation success.
\subsubsection{Limitations}
Entirely simulation-based; heavy computational costs from object detection.

\subsection{Adaptive Multi-Agent Reinforcement Learning [2]}
\subsubsection{Problem Statement}
Targets tactical conflict resolution (TCR) of UAVs within a shared urban airspace (decentralized UTM).
\subsubsection{Methodology}
MARL framework based on PPO with RNNs (CTDE paradigm).
\subsubsection{Innovations}
Centralized training with decentralized execution; handling varying intruder scenarios.
\subsubsection{Limitations}
Assumes full state observability and perfect communication.

\subsection{Curriculum-based DRL in 3D Urban Environments [3]}
\subsubsection{Problem Statement}
To navigate a broad expanse of 3D urban constructs with concerns on navigation and energy-optimization.
\subsubsection{Methodology}
Curriculum learning with Deep Q-Learning in simulated 3D maps.
\subsubsection{Innovations}
Stepwise difficulty increase improves generalization.
\subsubsection{Limitations}
No temporal modeling (LSTM), affecting dynamic performance.

\subsection{RLPlanNav (Hybrid DRL + Motion Planner) [4]}
\subsubsection{Problem Statement}
Combining high-level planning with low-level motion control.
\subsubsection{Methodology}
Top-level DRL agent combined with EGO-planner for trajectory generation.
\subsubsection{Innovations}
Hierarchical architecture complementing learning-based decision making with deterministic planning.
\subsubsection{Limitations}
Tested in static environments; doubts on dynamic applicability.

\subsection{Autonomous UAV Navigation with Adaptive Control (GARTD3) [5]}
\subsubsection{Problem Statement}
Adaptive control in 3D environments with velocity constraints.
\subsubsection{Methodology}
Guide Attention TD3 (GARTD3) with LSTM and attention module.
\subsubsection{Innovations}
Attention mechanism for navigation-obstacle avoidance balancing.
\subsubsection{Limitations}
High computational demands and sensitivity to hyperparameters.

\subsection{Multi-Level-Frontier Empowered Adaptive Path Planning [6]}
\subsubsection{Problem Statement}
Efficient path planning in unknown environments.
\subsubsection{Methodology}
Non-learning-based method using multi-level octree voxelization and Historical State Record Tree (HSRT).
\subsubsection{Innovations}
Multi-resolution voxel modeling.
\subsubsection{Limitations}
Heuristic nature lacks adaptability of DRL.

\subsection{APPA-3D: Autonomous 3D Path Planning [7]}
\subsubsection{Problem Statement}
Path planning in unknown complex 3D environments requiring collision avoidance and handling unknown information.
\subsubsection{Methodology}
APPA-3D algorithm using DRL with a dynamic reward function and improved exploration strategy based on action selection probability.
\subsubsection{Innovations}
Dynamic reward function tailored to the flight environment and optimized action exploration.
\subsubsection{Results}
Effective collision-free path planning from start to target in unknown 3D environments.
\subsubsection{Limitations}
Reliance on simulated environments for training.

\subsection{Agile DQN: Adaptive Deep Recurrent Attention RL [8]}
\subsubsection{Problem Statement}
UAV obstacle avoidance in 3D environments with high-dimensional inputs and partial observability.
\subsubsection{Methodology}
Agile DQN (AG-DQN) synergizing Glimpse Network, LSTM, Emission Network, and Q-Network.
\subsubsection{Innovations}
Dynamic focus on key visual features (Attention) and adaptive temporal attention strategy.
\subsubsection{Results}
Improved performance over existing DRL methods in navigation success and robustness.
\subsubsection{Limitations}
Complex architecture increases computational requirements.

\subsection{Autonomous UAV Visual Navigation Using Improved DRL [9]}
\subsubsection{Problem Statement}
Navigation in dynamic environments with moving objects; traditional DRL suffers from slow learning.
\subsubsection{Methodology}
Two-stage learning: (1) Reinforced learning (DQN), (2) Self-supervised learning (Contrastive loss) to fine-tune backbone.
\subsubsection{Innovations}
Integration of self-supervised learning to speed up encoding of input scenes.
\subsubsection{Results}
Faster learning and better navigation performance in dynamic situations.
\subsubsection{Limitations}
Two-stage training process adds complexity.

\subsection{SIGN: Safety-Aware Image-Goal Navigation [10]}
\subsubsection{Problem Statement}
Image-Goal Navigation in cluttered environments where collisions are costly.
\subsubsection{Methodology}
Safety-Aware Image-Goal Navigation (SIGN) using Reinforcement Learning with safety constraints.
\subsubsection{Innovations}
Safety-aware mechanism integrated into the navigation policy; end-to-end image-based navigation.
\subsubsection{Results}
Achieved navigation goals while minimizing collision risks.
\subsubsection{Limitations}
Image-based navigation can be sensitive to lighting; potential blind spots.

\subsection{Autonomous Localized Path Planning based on TD3 [11]}
\subsubsection{Problem Statement}
Local path planning in unfamiliar environments; issues with consistency and native controller influence.
\subsubsection{Methodology}
TD3 (Twin Delayed DDPG) strategy for continuous control.
\subsubsection{Innovations}
Application of TD3 strategy for autonomous local path planning addressing overestimation bias.
\subsubsection{Results}
Success rate of 93\% (no obstacles) and 92\% (with obstacles) in Gazebo simulations.
\subsubsection{Limitations}
Focuses on local planning; might need global planner integration.

\subsection{Deep RL Controller for USV Path Following [12]}
\subsubsection{Problem Statement}
Path following for Unmanned Surface Vehicles (USV) in complex environments.
\subsubsection{Methodology}
DDPG (Deep Deterministic Policy Gradient) with custom MDP.
\subsubsection{Innovations}
Validated through full-scale free-running tests of a USV.
\subsubsection{Results}
Successful path following in simulation and real-world tests.
\subsubsection{Limitations}
Application is USV (2D), not UAV (3D).

\subsection{Multi-Critic DDPG UAV Path Planning [13]}
\subsubsection{Problem Statement}
Environmental sensitivity and slow convergence in DDPG-based path planning.
\subsubsection{Methodology}
Improved DDPG with Multi-Critic architecture.
\subsubsection{Innovations}
Multi-critic mechanism to reduce variance and improve stability.
\subsubsection{Results}
Improved convergence speed and stability.
\subsubsection{Limitations}
Increased computational cost due to multiple critic networks.

\subsection{UAV Path Planning in Large-Scale Environments [14]}
\subsubsection{Problem Statement}
Path planning in large-scale, dynamic environments with limited sensors (POMDP).
\subsubsection{Methodology}
DRL with Recurrent Neural Network (RNN) and adaptive experience replay based on failure frequency.
\subsubsection{Innovations}
Use of RNN for temporal memory and adaptive experience replay.
\subsubsection{Results}
Significant improvements over DQN and DRQN in stability and efficiency.
\subsubsection{Limitations}
Partial observability challenges remain.

\subsection{Collaborative Path Planning to Avoid Sudden Threats [15]}
\subsubsection{Problem Statement}
Multi-UAV path planning with sudden threats requiring rapid re-planning.
\subsubsection{Methodology}
V-Diagram, mission assignment model, cubic spline, and crowding mechanism (Non-DRL).
\subsubsection{Innovations}
Combination of V-diagram and mission assignment; secondary security screening.
\subsubsection{Results}
Effective avoidance of sudden threats.
\subsubsection{Limitations}
Heuristic methods may not generalize as well as learning-based methods.

\subsection{Cooperative Control of UAV Swarm via Information Measures [16]}
\subsubsection{Problem Statement}
Decentralized control of UAV swarms for cooperative missions.
\subsubsection{Methodology}
Rule-based decentralized control using information theoretic measures (Non-DRL).
\subsubsection{Innovations}
Use of information measures to estimate value of future actions without central authority.
\subsubsection{Results}
Highly cooperative performance.
\subsubsection{Limitations}
Performance depends on task constraint complexity.

\section{Comparative Analysis}
This section compares all sixteen approaches based on their algorithms, environment types, and key innovations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{comparison_assets/algorithm_distribution.png}
\caption{Distribution of Algorithms across the 16 reviewed papers. DQN variants remain the most popular, followed by DDPG and TD3.}
\label{fig:algo_dist}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{comparison_assets/environment_focus.png}
\caption{Environment Focus: A significant portion of research addresses dynamic and unknown environments, highlighting the shift from static map-based planning.}
\label{fig:env_focus}
\end{figure}

Table I and Table II (next page) summarize the key attributes of the reviewed papers.

\begin{table*}[htbp]
\caption{Comparison of UAV Navigation Approaches (Expanded)}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Paper} & \textbf{Algorithm} & \textbf{Input/State} & \textbf{Environment} & \textbf{Key Innovation} \\
\hline
[1] Wang et al. & FRDDM-DQN & Visual & 3D Sim (Dynamic) & Faster R-CNN + DDM \\
\hline
[2] Fremond et al. & MARL (PPO) & Positional & Urban Airspace & Centralized Training \\
\hline
[3] Carvalho et al. & Curriculum DQN & 3D Map & 3D Urban & Curriculum Learning \\
\hline
[4] Xue \& Chen & Hybrid (DQN+Planner) & RGB + Partial Map & Static & Hierarchical Planning \\
\hline
[5] Yin et al. & GARTD3 & 3D Coords & Complex 3D & Attention Mechanism \\
\hline
[6] Duan et al. & Multi-Level Planner & Voxel Map & Unknown & Multi-resolution Voxel \\
\hline
[7] Wang et al. & APPA-3D (RL) & Sensors & Unknown 3D & Dynamic Reward \\
\hline
[8] AlMahamid et al. & Agile DQN & Visual & 3D Sim & Attention + LSTM \\
\hline
[9] Samma et al. & Improved DQN & Visual & Dynamic & Self-Supervised Learning \\
\hline
[10] Yan et al. & SIGN (RL) & Image & Cluttered & Safety-Awareness \\
\hline
[11] Zhao et al. & TD3 & Local State & Unfamiliar & TD3 Strategy \\
\hline
[12] Woo et al. & DDPG & USV State & Real USV & Real-world Validation \\
\hline
[13] Wu et al. & Multi-Critic DDPG & State & Complex & Multi-Critic \\
\hline
[14] Xie et al. & DRQN & POMDP & Large-Scale & Adaptive Replay \\
\hline
[15] Chen et al. & V-Diagram (Heuristic) & Threat Info & Multi-UAV & V-Diagram + Crowding \\
\hline
[16] Cheng et al. & Info Measures (Rule) & Local Info & Swarm & Information Theory \\
\hline
\end{tabular}
\end{center}
\end{table*}

\section{Conclusion}
This expanded review analyzed sixteen studies on UAV navigation, ranging from DRL-based methods like DQN, DDPG, and TD3 to heuristic and rule-based approaches. The trend shows a strong move towards handling dynamic and unknown environments using advanced mechanisms like Attention, Recurrent Neural Networks (LSTM/RNN), and Hybrid architectures. While simulation remains the primary testing ground, some studies [12] are bridging the gap to real-world deployment. Future research should focus on computational efficiency for on-board processing and more robust real-world validation.

\begin{thebibliography}{00}
\bibitem{b1} Wang et al., "Deep-reinforcement-learning-based UAV autonomous navigation..."
\bibitem{b2} Fremond et al., "Adaptive Multi-Agent Reinforcement Learning..."
\bibitem{b3} Carvalho et al., "UAV Navigation in 3D Urban Environments..."
\bibitem{b4} Xue and Chen, "Combining Motion Planner and Deep Reinforcement Learning..."
\bibitem{b5} Yin et al., "Autonomous UAV Navigation with Adaptive Control..."
\bibitem{b6} Duan et al., "Multi-Level-Frontier Empowered Adaptive Path Planning..."
\bibitem{b7} J. Wang et al., "APPA-3D: an autonomous 3D path planning algorithm...", Nature Sci Rep, 2024.
\bibitem{b8} F. AlMahamid et al., "Agile DQN: adaptive deep recurrent attention...", Nature Sci Rep, 2024.
\bibitem{b9} H. Samma et al., "Autonomous UAV Visual Navigation Using an Improved DRL", IEEE Access, 2024.
\bibitem{b10} Y. Yan et al., "SIGN: Safety-Aware Image-Goal Navigation...", Arxiv, 2025.
\bibitem{b11} Z. Feiyu et al., "Autonomous localized path planning... based on TD3", Nature Sci Rep, 2024.
\bibitem{b12} J. Woo et al., "Deep reinforcement learning-based controller for path following of a USV", Ocean Eng, 2019.
\bibitem{b13} R. Wu et al., "A multi-critic deep deterministic policy gradient UAV path planning", IEEE CIS, 2020.
\bibitem{b14} R. Xie et al., "UAV Path Planning... in Large-Scale and Dynamic Environments", IEEE Access, 2021.
\bibitem{b15} X. Chen et al., "Collaborative Path Planning... to Avoid Sudden Threats", Shenyang Aerospace.
\bibitem{b16} H. Cheng et al., "Cooperative control of UAV swarm via information measures", IJIUS.
\end{thebibliography}

\end{document}
